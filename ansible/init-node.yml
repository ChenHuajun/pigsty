#!/usr/bin/ansible-playbook
---
#==============================================================#
# File      :   init-node.yml
# Mtime     :   2020-03-24
# Desc      :   init node for postgres cluster
#               install necessary packages
#               init node hostname as <seq>.<cluster>
#               create postgres user with cluster ssh access
#               launch ntp node_exporter consul service
# Path      :   playbooks/init-node.yml
# Author    :   Vonng(fengruohang@outlook.com)
#==============================================================#
# Hosts :
#   works on group 'cluster' by default
#
# Template:
#     consul.json.j2 -> /etc/consul.d/consul.json
#==============================================================#
- name: Init Postgres Node Environment
  gather_facts: false
  any_errors_fatal: true
  serial: 10
  become: yes
  become_method: sudo

  hosts: cluster
  tasks:
    ################################################################
    # PHASE 1: [check] precondition
    ################################################################
    - name: Check cluster variable
      tags: [check]
      shell: "echo 'variable <cluster> is required for init procedure' && exit 1"
      delegate_to: localhost
      when: cluster is not defined

    # seq is cluster-wide unique incremental integer sequence to distinguish instances
    - name: Check instance variable
      tags: [check]
      shell: "echo 'instance variable <seq> is required for init procedure' && exit 1"
      delegate_to: localhost
      when: seq is not defined

    # [check] connectivity
    - name: Check connectivity
      tags: [check]
      action: ping

    ################################################################
    # PHASE 2: [setup] hostname to <seq>.<cluster>
    ################################################################
    # [setup] hostname
    - name: Build node hostname from cluster and id
      tags: [setup]
      set_fact: instance_name="{{ seq }}.{{ cluster }}"

    - name: Setup hostname
      tags: [setup]
      hostname:
        name: "{{ instance_name }}"


    ################################################################
    # PHASE 3: configure node using local pigsty yum repo
    ################################################################
    # [repo] local yum repo only
    - name: Configure with local yum
      tags: [repo]
      shell: 'yum-config-manager --disable \*; yum-config-manager --enable pigsty'


    ################################################################
    # PHASE 4: install common packages
    ################################################################
    # [install] packages
    - name: Install ntp, consul, node_exporter, pg_exporter and some utils
      tags: [install]
      yum:
        name:
          - ntp
          - node_exporter
          - pg_exporter
          - consul
          - etcd
          - lsof
          - wget
          - unzip
          - lz4
          - git
          - nc
          - pv
          - jq
          - sysstat
          - bind-utils
          - net-tools
          - keepalived


    ################################################################
    # PHASE 5: create dbsu postgres and cluster-wide ssh access
    ################################################################
    - name: Create os group postgres
      tags: [user]
      group:
        name: postgres
        gid: 256

    - name: Create os user postgres
      tags: [user]
      user:
        uid: 256
        name: postgres
        group: postgres
        password: postgres
        comment: postgres services
        create_home: yes
        home: /home/postgres

    - name: Create /home/postgres/.ssh
      tags: [user]
      file:
        path: /home/postgres/.ssh
        state: directory
        owner: postgres
        group: postgres
        mode: 0700

    - name: Create ssh key for cluster
      tags: [user]
      run_once: true
      delegate_to: localhost
      shell: |
        rm -rf "/tmp/{{ cluster }}.id_rsa" "/tmp/{{ cluster }}.id_rsa.pub"
        ssh-keygen -b 1024 -t rsa -q -N "" -f "/tmp/{{ cluster }}.id_rsa" <<< y
        sudo chown vagrant "/tmp/{{ cluster }}.id_rsa" "/tmp/{{ cluster }}.id_rsa.pub"

    - name: Copy ssh key to cluster
      tags: [user]
      copy:
        src: "/tmp/{{ cluster }}.{{ item }}"
        dest: "/home/postgres/.ssh/{{ item }}"
        owner: postgres
        group: postgres
        mode: 0600
      with_items:
        - id_rsa
        - id_rsa.pub

    - name: Create ssh key for cluster
      tags: [user]
      shell: |
        echo "StrictHostKeyChecking=no"    >> /home/postgres/.ssh/config
        cat /home/postgres/.ssh/id_rsa.pub >> /home/postgres/.ssh/authorized_keys
        chmod 600 /home/postgres/{id_rsa,id_rsa.pub,config}
        chmod 644 /home/postgres/authorized_keys
        chmod 700
        chown -R postgres:postgres /home/postgres



    ################################################################
    # PHASE 6: install ntp and sync time
    ################################################################
    # [ntp] service
    - name: Launch ntpd service
      tags: [ntp]
      systemd:
        name: ntpd
        state: started
        enabled: yes

    - name: Sync time
      tags: [ntp]
      shell: ntpdate -u pool.ntp.org



    ################################################################
    # PHASE 7: setup consul
    ################################################################
    # [clean] consul
    - name: Consul leave cluster if exists
      tags: [consul]
      ignore_errors: true
      shell: |
        if [[ $(ps aux | grep consul | grep agent | wc -l) != 0 ]]; then
            consul leave
            exit 0
        fi
        exit 0

    - name: Stop existing consul service
      tags: [consul]
      systemd:
        name: consul
        state: stopped
        enabled: no

    - name: Remove existing consul directory
      tags: [consul]
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /etc/consul.d
        - /var/lib/consul

    - name: Recreate /var/lib/consul directory
      tags: [consul]
      file:
        path: "{{ item }}"
        state: directory
        owner: consul
        group: postgres
        mode: 0770
      with_items:
        - /etc/consul.d
        - /var/lib/consul

    - name: Copy consul.json
      tags: [consul]
      template:
        src: "templates/consul.json.j2"
        dest: /etc/consul.d/consul.json
        owner: consul
        group: postgres
        mode: 0660

    - name: Create consul service
      tags: [consul]
      copy:
        content: |
          [Unit]
          Description="HashiCorp Consul - A service mesh solution"
          Documentation=https://www.consul.io/
          Requires=network-online.target
          After=network-online.target

          [Service]
          User=consul
          Group=consul
          ExecStart=/usr/bin/consul agent -config-dir=/etc/consul.d/
          ExecReload=/usr/bin/consul reload
          KillMode=process
          Restart=on-failure
          LimitNOFILE=65536

          [Install]
          WantedBy=multi-user.target
        dest: /usr/lib/systemd/system/consul.service
        owner: root
        group: root
        mode: 0644

    - name: Register consul service itself
      tags: [consul]
      copy:
        src: templates/srv-consul.json
        dest: /etc/consul.d/srv-consul.json
        owner: consul
        group: postgres
        mode: 0660

    - name: Launch consul service
      tags: [consul]
      systemd:
        name: consul
        state: started
        enabled: yes



    ################################################################
    # PHASE 8: install launch and register node_exporter
    ################################################################
    # [node_exporter] service
    - name: Launch node exporter service
      tags: [node_exporter]
      systemd:
        name: node_exporter
        state: restarted
        enabled: yes

    - name: Register node_exporter service
      tags: [node_exporter]
      copy:
        src: templates/srv-node_exporter.json
        dest: /etc/consul.d/srv-node_exporter.json
        owner: consul
        group: postgres
        mode: 0660

    - name: Launch consul service
      tags: [node_exporter]
      systemd:
        name: consul
        state: reloaded

